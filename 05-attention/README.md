# 05-Attention Mechanisms

This directory contains attention mechanism implementations leading to transformers.

## Structure
```
05-attention/
├── basic-attention/
│   ├── additive_attention.py
│   ├── multiplicative_attention.py
│   └── attention_visualization.py
├── transformers/
│   ├── self_attention.py
│   ├── multi_head_attention.py
│   ├── positional_encoding.py
│   └── transformer_block.py
└── implementations/
    ├── mini_transformer.py
    ├── encoder_decoder.py
    └── applications.py
```